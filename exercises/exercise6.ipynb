{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPLqK8AK7JS9mchL5gl2Tdl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spirosChv/imbizo2022/blob/main/exercises/exercise6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulating dendrites - Part 6: The Perceptron Algorithm"
      ],
      "metadata": {
        "id": "83xY-izo5uMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GnTNZ_-5riw"
      },
      "outputs": [],
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Make nicer plots -- Execute this cell\n",
        "def mystyle():\n",
        "  \"\"\"\n",
        "  Create custom plotting style.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  my_style : dict\n",
        "      Dictionary with matplotlib parameters.\n",
        "\n",
        "  \"\"\"\n",
        "  # color pallette\n",
        "  style = {\n",
        "      # Use LaTeX to write all text\n",
        "      \"text.usetex\": False,\n",
        "      \"font.family\": \"DejaVu Sans\",\n",
        "      \"font.weight\": \"bold\",\n",
        "      # Use 16pt font in plots, to match 16pt font in document\n",
        "      \"axes.labelsize\": 16,\n",
        "      \"axes.titlesize\": 20,\n",
        "      \"font.size\": 16,\n",
        "      # Make the legend/label fonts a little smaller\n",
        "      \"legend.fontsize\": 14,\n",
        "      \"xtick.labelsize\": 14,\n",
        "      \"ytick.labelsize\": 14,\n",
        "      \"axes.linewidth\": 2.5,\n",
        "      \"lines.markersize\": 10.0,\n",
        "      \"lines.linewidth\": 2.5,\n",
        "      \"xtick.major.width\": 2.2,\n",
        "      \"ytick.major.width\": 2.2,\n",
        "      \"axes.labelweight\": \"bold\",\n",
        "      \"axes.spines.right\": False,\n",
        "      \"axes.spines.top\": False\n",
        "  }\n",
        "\n",
        "  return style\n",
        "\n",
        "\n",
        "plt.style.use(\"seaborn-colorblind\")\n",
        "plt.rcParams.update(mystyle())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ogiFessE6l89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before start building the Perceptron Model, first we need to load the required packages and the data set. The data set is present in the sklearn datasets module."
      ],
      "metadata": {
        "id": "2MMi9hMm5tl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets.make_blobs(n_samples=1000, n_features=2,\n",
        "                           centers=2, cluster_std=2.2,\n",
        "                           random_state=2)\n",
        "# Plotting\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X[:, 0][y == 0], X[:, 1][y == 0], label='group1')\n",
        "plt.scatter(X[:, 0][y == 1], X[:, 1][y == 1], label='group2')\n",
        "plt.xlabel(\"feature 1\")\n",
        "plt.ylabel(\"feature 2\")\n",
        "plt.title('Random Classification Data with 2 classes')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1-6bJhbI6HO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two classes, red and green, and we want to separate them by drawing a straight line between them. Or, more formally, we want to learn a set of parameters theta to find an optimal hyperplane(straight line for our data) that separates the two classes."
      ],
      "metadata": {
        "id": "0firQW4j60bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s code the step function."
      ],
      "metadata": {
        "id": "71bfAuhL_r9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def step_func(z):\n",
        "  return 1.0 if (z > 0) else 0.0"
      ],
      "metadata": {
        "id": "k2OMfYNT60xG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visually understand the Perceptron by looking at the above image. For every training example, we first take the dot product of input features and parameters, theta. Then, we apply the Unit Step Function to make the prediction(`y_hat`).\n",
        "\n",
        "And if the prediction is wrong or in other words the model has misclassified that example, we make the update for the parameters theta. We don’t update when the prediction is correct (or the same as the true/target value y).\n",
        "\n",
        "Let’s see what the update rule is."
      ],
      "metadata": {
        "id": "dv1qMzhi7Le4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Update Rule\n",
        "\n",
        "The perception update rule is very similar to the Gradient Descent update rule. The following is the update rule:\n",
        "\n",
        "\\begin{equation}\n",
        "w_j := w_j + \\eta \\left( y^{[i]} - \\phi(x^{[i]}) \\right)x^{[i]}_j\n",
        "\\end{equation}\n",
        "\n",
        "where $j \\in [1, D]$, and $i \\in [1, N]$, $D$: number of features, $N$: number of samples.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Note:** Even though the Perceptron algorithm may look similar to logistic regression, it is actually a very different type of algorithm, since it is difficult to endow the perceptron’s predictions with meaningful probabilistic interpretations, or derive the perceptron as a maximum likelihood estimation algorithm.\n",
        "\n",
        "<br>\n",
        "\n",
        "**Math behind the perceptron algorithm:**\n",
        "\n",
        "We can distinguish the information flow into two phases:\n",
        "- forward pass\n",
        "- backward pass\n",
        "\n",
        "During the forward pass, we calculate the activation of the output node ($\\hat{y}$).\n",
        "\n",
        "\\begin{align}\n",
        "z_i^{[j]} &= \\sum_{i=1}^{D}w_ix_i^{[j]} + b \\\\\n",
        "\\hat{y}^{[j]} &= \\phi \\left( z_i^{[j]} \\right)\n",
        "\\end{align}\n",
        "\n",
        "where $i$ denotes the features, and $j$ the samples.\n",
        "\n",
        "Then, we calculate a loss $\\mathcal{L}$ (i.e., error) between our prediction $\\hat{y}$ and the real target value $y$.\n",
        "\n",
        "Here, we will use the squared error defined as:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathcal{L}(y, \\hat{y}; w, b) = \\left( y - \\hat{y} \\right)^2 = \\left( y -  \\phi \\left( z_i^{[j]} \\right) \\right)^2 = \\left( y - \\phi \\left( \\sum_{i=1}^{D}w_ix_i^{[j]} + b\\right) \\right)^2\n",
        "\\end{equation}\n",
        "\n",
        "In mathematics, a way to find the optimum (i.e., maximum or minimum) of any function with respect to a parameter is to calculate its derivative with respect to this parameter and try to minimize (or maximize) it. In our case, as we want to minimize the error between our predictions and the real target values, we will perform a minimization. Our parameters are the weights $w_i$ and the bias $b$.\n",
        "\n",
        "Our aim is to calculate the partial derivatives with respect to the parameters. We are going to use the chain rule, which is a formula that expresses the derivative of the composition of two differentiable functions $f$ and $g$ in terms of the derivatives of $f$ and $g$.\n",
        "\n",
        "If a variable $z$ depends on the variable $y$, which itself depends on the variable $x$ (that is, $y$ and $z$ are dependent variables), then $z$ depends on $x$ as well, via the intermediate variable $y$. In this case, the chain rule is expressed as\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\n",
        "\\end{equation}\n",
        "\n",
        "During the backword pass, we apply the chain rule, as we want to find the $\\frac{\\partial{\\mathcal{L}}}{\\partial{w_i}}$, and $\\frac{\\partial{\\mathcal{L}}}{\\partial{b}}$.\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial{\\mathcal{L}}}{\\partial{w_i}} &= \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{w_i}} = \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{z_i}}\\frac{\\partial{z_i}}{\\partial{w_i}} = -2(y-\\hat{y}) \\phi'(z_i)x_i \\\\ \\\\\n",
        "\\frac{\\partial{\\mathcal{L}}}{\\partial{b}} &= \\frac{\\partial{\\mathcal{L}}}{\\partial{\\hat{y}}} \\frac{\\partial{\\hat{y}}}{\\partial{b}} = -2(y-\\hat{y})\n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "RdJSYuUK_6F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron():\n",
        "\n",
        "  def __init__(self, lr=0.01, epochs=200, activ='threshold'):\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr\n",
        "    self.activ = activ\n",
        "\n",
        "  def activ_func(self, z, mode='threshold', deriv=False):\n",
        "    if mode == 'threshold':\n",
        "      if deriv:\n",
        "        return 1.0\n",
        "      else:\n",
        "        return np.where(z >= 0, 1, 0)\n",
        "    elif mode == 'sigmoid':\n",
        "      def sigmoid(z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "      if deriv:\n",
        "        return sigmoid(z) * (1 - sigmoid(z))\n",
        "      else:\n",
        "        return sigmoid(z)\n",
        "\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    N, D = X.shape\n",
        "    # Initialization of the weights\n",
        "    w = np.zeros((D, 1))\n",
        "    b = 0\n",
        "    losses = []\n",
        "    for i in tqdm(range(self.epochs)):\n",
        "      loss = 0\n",
        "      for idx, x_i in enumerate(X):\n",
        "        z_i = x_i.T @ w + b\n",
        "        yhat = self.activ_func(z_i, self.activ)\n",
        "\n",
        "        loss += (y[idx] - yhat)**2\n",
        "\n",
        "        # Update rule\n",
        "        w -= self.lr*(-2*(y[idx] - yhat) * self.activ_func(z_i, self.activ, deriv=True) * x_i).reshape(-1, 1)\n",
        "        b -= self.lr*(-2*(y[idx] - yhat))\n",
        "\n",
        "      losses.append(loss/X.shape[0])\n",
        "\n",
        "    return losses, w, b\n",
        "\n",
        "  def predict(self, X, y, w, b):\n",
        "    ypred = []\n",
        "    for x_i in X:\n",
        "      a_i = self.activ_func(x_i.T @ w + b, self.activ)\n",
        "      ypred.append(1 if a_i >= 0.5 else 0)\n",
        "\n",
        "    accuracy = np.sum(np.abs(ypred - y) == 0)/len(y)\n",
        "    return ypred, accuracy"
      ],
      "metadata": {
        "id": "Ea4Os6I57LFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2,\n",
        "                                                random_state=0)"
      ],
      "metadata": {
        "id": "KjfylDyMZuDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's initialize a new perceptron class object. Then we will use that object we will call `fit` method on our training data to learn the best possible parameters. We will evaluate the model performance on the test data by calculating the testing accuracy."
      ],
      "metadata": {
        "id": "ZZ0izPzf8MfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# draw boundary line\n",
        "def plot_decision_boundary(X, w, b):\n",
        "  c = -b/w[1]\n",
        "  m = -w[0]/w[1]\n",
        "  x1min, x1max = [np.min(X[:, 0]), np.max(X[:, 0])]\n",
        "  x2min, x2max = [np.min(X[:, 1]), np.max(X[:, 1])]\n",
        "  xd = np.array([x1min, x1max])\n",
        "  yd = m*xd + c\n",
        "  # Plotting\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.scatter(X[y==0, 0], X[y==0, 1], label='group 1')\n",
        "  plt.scatter(X[y==0, 1], X[y==1, 1], label='group 2')\n",
        "  plt.xlabel(\"feature 1\")\n",
        "  plt.ylabel(\"feature 2\")\n",
        "  plt.title('Perceptron Algorithm')\n",
        "  plt.plot(xd, yd, 'k-')\n",
        "  plt.fill_between(xd, yd, alpha=0.1)\n",
        "  plt.fill_between(xd, yd, alpha=0.1)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "4WLD_D8A77oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percp = Perceptron(lr=1e-2, epochs=500, activ='sigmoid')\n",
        "\n",
        "losses, w, b = percp.fit(Xtrain, ytrain)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (a.u.)')\n",
        "plt.show()\n",
        "\n",
        "ypred, accuracy = percp.predict(Xtest, ytest, w, b)\n",
        "print(f\"Accuracy on test set: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "4ax20P3_-cO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(X, w, b)"
      ],
      "metadata": {
        "id": "Ghu5Hw7balIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Explore hyperparameters\n",
        "\n",
        "std = 4  # @param\n",
        "lr = 0.1  # @param\n",
        "epochs = 200  # @param\n",
        "func = \"sigmoid\"  # @param\n",
        "\n",
        "X, y = datasets.make_blobs(n_samples=150, n_features=2,\n",
        "                           centers=2, cluster_std=std,\n",
        "                           random_state=2)\n",
        "\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2,\n",
        "                                                random_state=0)\n",
        "\n",
        "percp = Perceptron(lr=lr, epochs=epochs, activ=func)\n",
        "\n",
        "losses, w, b = percp.fit(Xtrain, ytrain)\n",
        "\n",
        "ypred, accuracy = percp.predict(Xtest, ytest, w, b)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(range(len(losses)), losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss (a.u.)')\n",
        "plt.title(f'Accuracy: {accuracy*100:.2f}%')\n",
        "plt.show()\n",
        "plot_decision_boundary(X, w, b)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6cNHhd5neZoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "# Model / data parameters\n",
        "num_classes = 2\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Choose two classes, 0 and 1\n",
        "\n",
        "# Load the data and split it between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "train_mask = np.isin(y_train, [0, 1])\n",
        "test_mask = np.isin(y_test, [0, 1])\n",
        "\n",
        "x_train, y_train = x_train[train_mask], y_train[train_mask]\n",
        "x_test, y_test = x_test[test_mask], y_test[test_mask]\n",
        "\n",
        "# Scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255"
      ],
      "metadata": {
        "id": "ECBgMok1uuH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "idxs = np.random.choice(x_train.shape[0], 10, replace=False)\n",
        "for i, idx in enumerate(idxs):\n",
        "  plt.subplot(5, 5, i+1)\n",
        "  plt.imshow(x_train[idx], 'gray')\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OyRVV8qAxAGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2])\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])"
      ],
      "metadata": {
        "id": "40r8AB8yy8mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percp = Perceptron(lr=1e-2, epochs=220, activ='sigmoid')\n",
        "\n",
        "losses, w, b = percp.fit(x_train, y_train)\n",
        "\n",
        "ypred, accuracy = percp.predict(x_test, y_test, w, b)\n",
        "\n",
        "print(f\"Accuracy on test set:{accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "cWC2oLGdyWZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy on test set: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "y7uSlml1_sSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limitations of Perceptron Algorithm\n",
        "\n",
        "- It is only a linear classifier, can never separate data that are not linearly separable.\n",
        "- The algorithm is used only for Binary Classification problems."
      ],
      "metadata": {
        "id": "gj4-_8cUBf2y"
      }
    }
  ]
}